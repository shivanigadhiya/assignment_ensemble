{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: What is Gradient Boosting Regression?\n",
    "\n",
    "Definition:\n",
    "\n",
    "Gradient Boosting Regression is an ensemble learning technique that builds a predictive model by combining multiple weak learners, typically decision trees, in a sequential manner to improve accuracy.\n",
    "Mechanism:\n",
    "\n",
    "It optimizes a loss function by iteratively adding weak learners that correct the errors of the previous learners, using gradient descent to minimize the loss.\n",
    "Q2: Implement a Simple Gradient Boosting Algorithm from Scratch Using Python and NumPy\n",
    "\n",
    "Here is a simple implementation of Gradient Boosting Regression:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class SimpleGradientBoosting:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the model with the mean of the target values\n",
    "        y_pred = np.full(y.shape, np.mean(y))\n",
    "        self.trees = []\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute the residuals\n",
    "            residuals = y - y_pred\n",
    "            \n",
    "            # Fit a decision tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            # Update predictions\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.full(X.shape[0], np.mean([tree.predict(X) for tree in self.trees]), dtype=float)\n",
    "        for tree in self.trees:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a simple dataset\n",
    "    X = np.array([[1], [2], [3], [4], [5]])\n",
    "    y = np.array([1.5, 1.7, 3.0, 3.5, 5.0])\n",
    "\n",
    "    # Train the model\n",
    "    model = SimpleGradientBoosting(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    print(f'Mean Squared Error: {mse}')\n",
    "    print(f'R-squared: {r2}')\n",
    "Q3: Experiment with Different Hyperparameters\n",
    "\n",
    "To optimize the performance of the model, you can use grid search or random search. Hereâ€™s an example using GridSearchCV from sklearn:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Create a simple dataset\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([1.5, 1.7, 3.0, 3.5, 5.0])\n",
    "\n",
    "# Define the model\n",
    "gb_model = GradientBoostingRegressor()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "Q4: What is a Weak Learner in Gradient Boosting?\n",
    "\n",
    "Definition:\n",
    "A weak learner is a model that performs slightly better than random guessing. In the context of Gradient Boosting, weak learners are typically shallow decision trees (often called \"stumps\") that are combined to form a strong predictive model.\n",
    "Q5: What is the Intuition Behind the Gradient Boosting Algorithm?\n",
    "\n",
    "Intuition:\n",
    "The intuition behind Gradient Boosting is to iteratively improve the model by focusing on the errors made by previous models. Each new learner is trained to predict the residuals (errors) of the combined predictions of all previous learners, effectively correcting the mistakes of the ensemble.\n",
    "Q6: How Does Gradient Boosting Algorithm Build an Ensemble of Weak Learners?\n",
    "\n",
    "Sequential Learning:\n",
    "Gradient Boosting builds an ensemble by training weak learners\n",
    "\n",
    "\n",
    "\n",
    "Share\n",
    "New\n",
    "Edit\n",
    "Continue\n",
    "Bookmark message\n",
    "Copy message\n",
    "Verify Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
